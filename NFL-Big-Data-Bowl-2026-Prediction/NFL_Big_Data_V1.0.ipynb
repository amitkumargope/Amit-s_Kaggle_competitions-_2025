{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832377f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing catboost...\n",
      "✓ catboost installed successfully\n",
      "✓ catboost installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['catboost']\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✓ {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", package])\n",
    "        print(f\"✓ {package} installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8e4df",
   "metadata": {},
   "source": [
    "# NFL Big Data Bowl 2026 - Advanced Player Trajectory Prediction\n",
    "\n",
    "## Objective\n",
    "Predict future player positions (x, y coordinates) on the field using tracking data from the 2023 NFL season.\n",
    "\n",
    "## Key Improvements Over Baseline:\n",
    "1. **Advanced Feature Engineering**: \n",
    "   - Enhanced physics-based features (momentum, energy, force projections)\n",
    "   - Temporal patterns and trajectory curvature\n",
    "   - Player interaction features\n",
    "   - Ball-relative coordinate system\n",
    "   \n",
    "2. **Multi-Model Ensemble**:\n",
    "   - LightGBM + XGBoost + CatBoost\n",
    "   - Role-specific models (Targeted Receiver, Defensive Coverage)\n",
    "   - Residual learning approach\n",
    "   \n",
    "3. **Better Validation**:\n",
    "   - Time-aware cross-validation\n",
    "   - Horizon-weighted loss\n",
    "   - Group-based splitting to prevent leakage\n",
    "\n",
    "4. **Optimization**:\n",
    "   - GPU acceleration\n",
    "   - Parallel data loading\n",
    "   - Memory-efficient processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fea9885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Working Directory: d:\\ML Practice\\NFL Big data\n",
      "Data Directory: d:\\ML Practice\\NFL Big data\\nfl-big-data-bowl-2026-prediction\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# NFL Big Data Bowl 2026 — Enhanced Multi-Model Approach\n",
    "# ============================================================\n",
    "\n",
    "import os, warnings, math, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool as MP, cpu_count\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "# Gradient Boosting libraries\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool as CatPool\n",
    "try:\n",
    "    from catboost.utils import get_gpu_device_count\n",
    "except:\n",
    "    def get_gpu_device_count(): return 0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "BASEDIR = Path(r\"d:\\ML Practice\\NFL Big data\\nfl-big-data-bowl-2026-prediction\")\n",
    "SAVE_DIR = Path(r\"d:\\ML Practice\\NFL Big data\")\n",
    "\n",
    "N_WEEKS = 18\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Cross-validation settings\n",
    "N_FOLDS = 5\n",
    "USE_GROUP_KFOLD = True\n",
    "\n",
    "# Model hyperparameters\n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 127,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 20,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 10.0,\n",
    "    'verbose': -1,\n",
    "    'device': 'gpu' if get_gpu_device_count() > 0 else 'cpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "}\n",
    "\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 10.0,\n",
    "    'tree_method': 'gpu_hist' if get_gpu_device_count() > 0 else 'hist',\n",
    "    'predictor': 'gpu_predictor' if get_gpu_device_count() > 0 else 'cpu_predictor',\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "CAT_PARAMS = {\n",
    "    'iterations': 15000,\n",
    "    'learning_rate': 0.08,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 8.0,\n",
    "    'task_type': 'GPU' if get_gpu_device_count() > 0 else 'CPU',\n",
    "    'loss_function': 'RMSE',\n",
    "    'early_stopping_rounds': 500,\n",
    "    'verbose': 200,\n",
    "    'random_seed': SEED,\n",
    "    'border_count': 254,\n",
    "}\n",
    "\n",
    "# Feature engineering parameters\n",
    "K_NEIGHBORS = 8  # Number of neighbors for interaction features\n",
    "RADIUS = 35.0    # Interaction radius in yards\n",
    "HORIZON_WEIGHT = 0.3  # Weight factor for longer horizons\n",
    "\n",
    "print(f\"GPU Available: {get_gpu_device_count() > 0}\")\n",
    "print(f\"Working Directory: {SAVE_DIR}\")\n",
    "print(f\"Data Directory: {BASEDIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba947c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_week(week_num: int):\n",
    "    \"\"\"Load input and output data for a specific week\"\"\"\n",
    "    fin = BASEDIR / f\"train/input_2023_w{week_num:02d}.csv\"\n",
    "    fout = BASEDIR / f\"train/output_2023_w{week_num:02d}.csv\"\n",
    "    return pd.read_csv(fin), pd.read_csv(fout)\n",
    "\n",
    "def load_all_train():\n",
    "    \"\"\"Load all training data in parallel\"\"\"\n",
    "    print(\"Loading training data...\")\n",
    "    with MP(min(cpu_count(), N_WEEKS)) as pool:\n",
    "        res = list(tqdm(pool.imap(load_week, range(1, N_WEEKS+1)), total=N_WEEKS, desc=\"Loading weeks\"))\n",
    "    \n",
    "    train_input = pd.concat([r[0] for r in res], ignore_index=True)\n",
    "    train_output = pd.concat([r[1] for r in res], ignore_index=True)\n",
    "    \n",
    "    print(f\"Train input shape:  {train_input.shape}\")\n",
    "    print(f\"Train output shape: {train_output.shape}\")\n",
    "    print(f\"Memory usage: {train_input.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return train_input, train_output\n",
    "\n",
    "# Utility functions\n",
    "def to_inches(height_str):\n",
    "    \"\"\"Convert height string (e.g., '6-1') to inches\"\"\"\n",
    "    try:\n",
    "        feet, inches = str(height_str).split(\"-\")\n",
    "        return float(feet) * 12.0 + float(inches)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2601bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_physics_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create physics-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure required columns exist with defaults\n",
    "    for col in ['x', 'y', 's', 'a', 'o', 'dir', 'ball_land_x', 'ball_land_y']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    \n",
    "    # Player physical attributes\n",
    "    df['height_inches'] = df.get('player_height', '6-0').apply(to_inches).fillna(72.0)\n",
    "    df['weight_lbs'] = pd.to_numeric(df.get('player_weight', 200), errors='coerce').fillna(200.0)\n",
    "    df['bmi'] = (df['weight_lbs'] / (df['height_inches']**2 + 1e-6)) * 703.0\n",
    "    \n",
    "    # Direction and heading (dir=0° points to +y axis)\n",
    "    dir_rad = np.radians(pd.to_numeric(df['dir'], errors='coerce').fillna(0.0))\n",
    "    df['heading_x'] = np.sin(dir_rad)\n",
    "    df['heading_y'] = np.cos(dir_rad)\n",
    "    \n",
    "    # Velocity components\n",
    "    speed = pd.to_numeric(df['s'], errors='coerce').fillna(0.0)\n",
    "    accel = pd.to_numeric(df['a'], errors='coerce').fillna(0.0)\n",
    "    df['velocity_x'] = speed * df['heading_x']\n",
    "    df['velocity_y'] = speed * df['heading_y']\n",
    "    df['acceleration_x'] = accel * df['heading_x']\n",
    "    df['acceleration_y'] = accel * df['heading_y']\n",
    "    \n",
    "    # Ball-relative geometry\n",
    "    pos_x = pd.to_numeric(df['x'], errors='coerce').fillna(0.0)\n",
    "    pos_y = pd.to_numeric(df['y'], errors='coerce').fillna(0.0)\n",
    "    ball_x = pd.to_numeric(df['ball_land_x'], errors='coerce').fillna(0.0)\n",
    "    ball_y = pd.to_numeric(df['ball_land_y'], errors='coerce').fillna(0.0)\n",
    "    \n",
    "    dx = ball_x - pos_x\n",
    "    dy = ball_y - pos_y\n",
    "    dist = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    df['dist_to_ball'] = dist\n",
    "    df['angle_to_ball'] = np.arctan2(dy, dx)\n",
    "    \n",
    "    # Ball-frame coordinate system (parallel and perpendicular to ball direction)\n",
    "    unit_x = dx / (dist + 1e-6)\n",
    "    unit_y = dy / (dist + 1e-6)\n",
    "    perp_x = -unit_y\n",
    "    perp_y = unit_x\n",
    "    \n",
    "    # Velocity projections in ball frame\n",
    "    df['v_parallel'] = df['velocity_x'] * unit_x + df['velocity_y'] * unit_y\n",
    "    df['v_perpendicular'] = df['velocity_x'] * perp_x + df['velocity_y'] * perp_y\n",
    "    \n",
    "    # Acceleration projections in ball frame\n",
    "    df['a_parallel'] = df['acceleration_x'] * unit_x + df['acceleration_y'] * unit_y\n",
    "    df['a_perpendicular'] = df['acceleration_x'] * perp_x + df['acceleration_y'] * perp_y\n",
    "    \n",
    "    # Heading alignment with ball direction\n",
    "    df['heading_alignment'] = df['heading_x'] * unit_x + df['heading_y'] * unit_y\n",
    "    \n",
    "    # Advanced physics\n",
    "    df['speed_squared'] = speed**2\n",
    "    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n",
    "    df['momentum_x'] = df['weight_lbs'] * df['velocity_x']\n",
    "    df['momentum_y'] = df['weight_lbs'] * df['velocity_y']\n",
    "    df['momentum_magnitude'] = np.sqrt(df['momentum_x']**2 + df['momentum_y']**2)\n",
    "    df['kinetic_energy'] = 0.5 * df['weight_lbs'] * df['speed_squared']\n",
    "    \n",
    "    # Orientation features\n",
    "    orient_rad = np.radians(pd.to_numeric(df['o'], errors='coerce').fillna(0.0))\n",
    "    df['orient_x'] = np.sin(orient_rad)\n",
    "    df['orient_y'] = np.cos(orient_rad)\n",
    "    df['orient_ball_align'] = df['orient_x'] * unit_x + df['orient_y'] * unit_y\n",
    "    \n",
    "    # Angle differences\n",
    "    df['dir_orient_diff'] = np.abs(df['dir'] - df['o'])\n",
    "    df['dir_orient_diff'] = np.minimum(df['dir_orient_diff'], 360 - df['dir_orient_diff'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d22794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add lag features, rolling statistics, and trajectory patterns\"\"\"\n",
    "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']).copy()\n",
    "    group_cols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    # Lag features (previous timesteps)\n",
    "    lag_features = ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a', \n",
    "                    'v_parallel', 'v_perpendicular', 'a_parallel', 'a_perpendicular']\n",
    "    \n",
    "    for lag in [1, 2, 3, 5]:\n",
    "        for col in lag_features:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n",
    "    \n",
    "    # Rolling statistics (smoothed trends)\n",
    "    rolling_features = ['x', 'y', 'velocity_x', 'velocity_y', 's', 'v_parallel', 'v_perpendicular']\n",
    "    \n",
    "    for window in [3, 5, 7]:\n",
    "        for col in rolling_features:\n",
    "            if col in df.columns:\n",
    "                rolling = df.groupby(group_cols)[col].rolling(window, min_periods=1)\n",
    "                df[f'{col}_rolling_mean_{window}'] = rolling.mean().reset_index(level=[0,1,2], drop=True)\n",
    "                df[f'{col}_rolling_std_{window}'] = rolling.std().reset_index(level=[0,1,2], drop=True)\n",
    "    \n",
    "    # Delta features (changes between frames)\n",
    "    delta_features = ['velocity_x', 'velocity_y', 'v_parallel', 'v_perpendicular', \n",
    "                      's', 'a', 'dist_to_ball']\n",
    "    \n",
    "    for col in delta_features:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_delta'] = df.groupby(group_cols)[col].diff()\n",
    "            df[f'{col}_delta2'] = df.groupby(group_cols)[f'{col}_delta'].diff()  # second derivative\n",
    "    \n",
    "    # Trajectory curvature (change in direction)\n",
    "    if 'angle_to_ball' in df.columns:\n",
    "        df['angle_to_ball_delta'] = df.groupby(group_cols)['angle_to_ball'].diff()\n",
    "    \n",
    "    # Acceleration patterns\n",
    "    if 'accel_magnitude' in df.columns:\n",
    "        df['accel_jerk'] = df.groupby(group_cols)['accel_magnitude'].diff()  # rate of change of acceleration\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe47d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_formation_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add team formation and spatial relationship features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Team-level statistics\n",
    "    grp = df.groupby(['game_id', 'play_id', 'frame_id', 'player_side'], sort=False)\n",
    "    \n",
    "    df['team_centroid_x'] = grp['x'].transform('mean')\n",
    "    df['team_centroid_y'] = grp['y'].transform('mean')\n",
    "    df['team_spread_x'] = grp['x'].transform('std').fillna(0.0)\n",
    "    df['team_spread_y'] = grp['y'].transform('std').fillna(0.0)\n",
    "    df['team_width'] = grp['y'].transform(lambda x: x.max() - x.min()).fillna(0.0)\n",
    "    df['team_length'] = grp['x'].transform(lambda x: x.max() - x.min()).fillna(0.0)\n",
    "    \n",
    "    # Player position relative to team centroid\n",
    "    df['rel_centroid_x'] = df['x'] - df['team_centroid_x']\n",
    "    df['rel_centroid_y'] = df['y'] - df['team_centroid_y']\n",
    "    df['dist_from_centroid'] = np.sqrt(df['rel_centroid_x']**2 + df['rel_centroid_y']**2)\n",
    "    \n",
    "    # Formation orientation (team bearing toward ball)\n",
    "    ball_x = pd.to_numeric(df['ball_land_x'], errors='coerce').fillna(0.0)\n",
    "    ball_y = pd.to_numeric(df['ball_land_y'], errors='coerce').fillna(0.0)\n",
    "    \n",
    "    bearing = np.arctan2(ball_y - df['team_centroid_y'], ball_x - df['team_centroid_x'])\n",
    "    df['formation_bearing_sin'] = np.sin(bearing)\n",
    "    df['formation_bearing_cos'] = np.cos(bearing)\n",
    "    \n",
    "    # Density features (how clustered is the team)\n",
    "    df['team_compactness'] = df['team_spread_x'] * df['team_spread_y']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e450e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighbor_features(df: pd.DataFrame, k_neighbors: int = K_NEIGHBORS, \n",
    "                             radius: float = RADIUS) -> pd.DataFrame:\n",
    "    \"\"\"Compute features based on nearby players (allies and opponents)\"\"\"\n",
    "    \n",
    "    cols_needed = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y',\n",
    "                   'velocity_x', 'velocity_y', 'player_side']\n",
    "    \n",
    "    src = df[cols_needed].copy()\n",
    "    \n",
    "    # Get last frame for each player\n",
    "    last_frame = (src.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "                     .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False)\n",
    "                     .tail(1)\n",
    "                     .rename(columns={'frame_id': 'last_frame_id'})\n",
    "                     .reset_index(drop=True))\n",
    "    \n",
    "    # Create neighbor pairs\n",
    "    neighbors = last_frame.merge(\n",
    "        src.rename(columns={\n",
    "            'frame_id': 'nb_frame_id', 'nfl_id': 'nfl_id_nb', \n",
    "            'x': 'x_nb', 'y': 'y_nb',\n",
    "            'velocity_x': 'vx_nb', 'velocity_y': 'vy_nb',\n",
    "            'player_side': 'player_side_nb'\n",
    "        }),\n",
    "        left_on=['game_id', 'play_id', 'last_frame_id'],\n",
    "        right_on=['game_id', 'play_id', 'nb_frame_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Remove self-pairs\n",
    "    neighbors = neighbors[neighbors['nfl_id_nb'] != neighbors['nfl_id']]\n",
    "    \n",
    "    # Calculate distances and relative velocities\n",
    "    neighbors['dx'] = neighbors['x_nb'] - neighbors['x']\n",
    "    neighbors['dy'] = neighbors['y_nb'] - neighbors['y']\n",
    "    neighbors['dvx'] = neighbors['vx_nb'] - neighbors['velocity_x']\n",
    "    neighbors['dvy'] = neighbors['vy_nb'] - neighbors['velocity_y']\n",
    "    neighbors['dist'] = np.sqrt(neighbors['dx']**2 + neighbors['dy']**2)\n",
    "    \n",
    "    # Filter by distance\n",
    "    neighbors = neighbors[np.isfinite(neighbors['dist']) & (neighbors['dist'] > 1e-6)]\n",
    "    if radius is not None:\n",
    "        neighbors = neighbors[neighbors['dist'] <= radius]\n",
    "    \n",
    "    # Identify allies vs opponents\n",
    "    neighbors['is_ally'] = (neighbors['player_side_nb'].fillna('') == \n",
    "                            neighbors['player_side'].fillna('')).astype(np.float32)\n",
    "    \n",
    "    # Rank neighbors by distance\n",
    "    keys = ['game_id', 'play_id', 'nfl_id']\n",
    "    neighbors['rank'] = neighbors.groupby(keys)['dist'].rank(method='first')\n",
    "    \n",
    "    if k_neighbors is not None:\n",
    "        neighbors = neighbors[neighbors['rank'] <= k_neighbors]\n",
    "    \n",
    "    # Weighted aggregation (closer neighbors have more weight)\n",
    "    tau = 8.0  # decay parameter\n",
    "    neighbors['weight'] = np.exp(-neighbors['dist'] / tau)\n",
    "    sum_weight = neighbors.groupby(keys)['weight'].transform('sum')\n",
    "    neighbors['weight_norm'] = np.where(sum_weight > 0, neighbors['weight'] / sum_weight, 0.0)\n",
    "    \n",
    "    neighbors['weight_ally'] = neighbors['weight_norm'] * neighbors['is_ally']\n",
    "    neighbors['weight_opp'] = neighbors['weight_norm'] * (1.0 - neighbors['is_ally'])\n",
    "    \n",
    "    # Weighted features\n",
    "    for col in ['dx', 'dy', 'dvx', 'dvy']:\n",
    "        neighbors[f'{col}_ally_w'] = neighbors[col] * neighbors['weight_ally']\n",
    "        neighbors[f'{col}_opp_w'] = neighbors[col] * neighbors['weight_opp']\n",
    "    \n",
    "    # Separate distances for allies and opponents\n",
    "    neighbors['dist_ally'] = np.where(neighbors['is_ally'] > 0.5, neighbors['dist'], np.nan)\n",
    "    neighbors['dist_opp'] = np.where(neighbors['is_ally'] < 0.5, neighbors['dist'], np.nan)\n",
    "    \n",
    "    # Aggregate neighbor features\n",
    "    agg_features = neighbors.groupby(keys).agg(\n",
    "        neighbor_ally_dx_mean=('dx_ally_w', 'sum'),\n",
    "        neighbor_ally_dy_mean=('dy_ally_w', 'sum'),\n",
    "        neighbor_ally_dvx_mean=('dvx_ally_w', 'sum'),\n",
    "        neighbor_ally_dvy_mean=('dvy_ally_w', 'sum'),\n",
    "        neighbor_opp_dx_mean=('dx_opp_w', 'sum'),\n",
    "        neighbor_opp_dy_mean=('dy_opp_w', 'sum'),\n",
    "        neighbor_opp_dvx_mean=('dvx_opp_w', 'sum'),\n",
    "        neighbor_opp_dvy_mean=('dvy_opp_w', 'sum'),\n",
    "        neighbor_ally_count=('is_ally', 'sum'),\n",
    "        neighbor_opp_count=('is_ally', lambda s: float(len(s) - s.sum())),\n",
    "        neighbor_ally_dist_min=('dist_ally', 'min'),\n",
    "        neighbor_ally_dist_mean=('dist_ally', 'mean'),\n",
    "        neighbor_opp_dist_min=('dist_opp', 'min'),\n",
    "        neighbor_opp_dist_mean=('dist_opp', 'mean'),\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Get distances to closest 3 neighbors\n",
    "    nearest = neighbors[neighbors['rank'] <= 3][keys + ['rank', 'dist']].copy()\n",
    "    nearest['rank'] = nearest['rank'].astype(int)\n",
    "    nearest_wide = nearest.pivot_table(index=keys, columns='rank', values='dist', aggfunc='first')\n",
    "    nearest_wide = nearest_wide.rename(columns={1: 'neighbor_d1', 2: 'neighbor_d2', 3: 'neighbor_d3'}).reset_index()\n",
    "    \n",
    "    agg_features = agg_features.merge(nearest_wide, on=keys, how='left')\n",
    "    \n",
    "    # Fill NaN values\n",
    "    for col in agg_features.columns:\n",
    "        if col.startswith('neighbor_'):\n",
    "            if 'count' in col:\n",
    "                agg_features[col] = agg_features[col].fillna(0.0)\n",
    "            elif 'dist' in col or 'd1' in col or 'd2' in col or 'd3' in col:\n",
    "                agg_features[col] = agg_features[col].fillna(radius if radius else 35.0)\n",
    "            else:\n",
    "                agg_features[col] = agg_features[col].fillna(0.0)\n",
    "    \n",
    "    return agg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a91c628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_role_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add role-specific binary indicators\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Player role indicators\n",
    "    player_role = df.get('player_role', '').astype(str)\n",
    "    player_side = df.get('player_side', '').astype(str)\n",
    "    \n",
    "    df['role_targeted_receiver'] = (player_role == 'Targeted Receiver').astype(np.int8)\n",
    "    df['role_defensive_coverage'] = (player_role == 'Defensive Coverage').astype(np.int8)\n",
    "    df['role_passer'] = (player_role == 'Passer').astype(np.int8)\n",
    "    df['side_offense'] = (player_side == 'Offense').astype(np.int8)\n",
    "    df['side_defense'] = (player_side == 'Defense').astype(np.int8)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f1cc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_rows(input_df: pd.DataFrame, output_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge input features (last observed frame) with output targets (future frames).\n",
    "    Each row represents a prediction task: given last observed state, predict future position.\n",
    "    \"\"\"\n",
    "    # Get last frame observation for each player\n",
    "    last_observations = (\n",
    "        input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "                .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False)\n",
    "                .tail(1)\n",
    "                .rename(columns={'frame_id': 'last_frame_id'})\n",
    "                .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Prepare output targets\n",
    "    output = output_df.copy()\n",
    "    output = output.rename(columns={'x': 'target_x', 'y': 'target_y'})\n",
    "    output['id'] = (\n",
    "        output['game_id'].astype(str) + '_' +\n",
    "        output['play_id'].astype(str) + '_' +\n",
    "        output['nfl_id'].astype(str) + '_' +\n",
    "        output['frame_id'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # Merge observations with targets\n",
    "    merged = output.merge(\n",
    "        last_observations, \n",
    "        on=['game_id', 'play_id', 'nfl_id'], \n",
    "        how='left',\n",
    "        suffixes=('', '_last')\n",
    "    )\n",
    "    \n",
    "    # Calculate time delta (frames into the future)\n",
    "    merged['delta_frames'] = (merged['frame_id'] - merged['last_frame_id']).clip(lower=0).astype(float)\n",
    "    merged['delta_time'] = merged['delta_frames'] / 10.0  # Convert to seconds (10 fps)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6b9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_acceleration_baseline(x_last, y_last, vx_last, vy_last, delta_t, ax_last, ay_last):\n",
    "    \"\"\"\n",
    "    Physics-based baseline: constant acceleration model\n",
    "    x(t) = x0 + v0*t + 0.5*a*t^2\n",
    "    \"\"\"\n",
    "    pred_x = x_last + vx_last * delta_t + 0.5 * ax_last * (delta_t ** 2)\n",
    "    pred_y = y_last + vy_last * delta_t + 0.5 * ay_last * (delta_t ** 2)\n",
    "    \n",
    "    # Clip to field boundaries\n",
    "    pred_x = np.clip(pred_x, 0.0, 120.0)\n",
    "    pred_y = np.clip(pred_y, 0.0, 53.3)\n",
    "    \n",
    "    return pred_x, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f16efb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_models(X, y_x, y_y, sample_weights=None, groups=None, \n",
    "                         base_x=None, base_y=None, use_lgb=True, use_xgb=True, use_cat=True):\n",
    "    \"\"\"\n",
    "    Train an ensemble of gradient boosting models with cross-validation.\n",
    "    Uses residual learning: models predict correction to baseline, not absolute position.\n",
    "    \"\"\"\n",
    "    \n",
    "    models_x_lgb, models_y_lgb = [], []\n",
    "    models_x_xgb, models_y_xgb = [], []\n",
    "    models_x_cat, models_y_cat = [], []\n",
    "    fold_scores = []\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    if USE_GROUP_KFOLD and groups is not None:\n",
    "        cv = GroupKFold(n_splits=N_FOLDS)\n",
    "        splits = list(cv.split(X, groups=groups))\n",
    "    else:\n",
    "        cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "        splits = list(cv.split(X))\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splits, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold_idx}/{N_FOLDS} - Train: {len(train_idx):,} | Val: {len(val_idx):,}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        yx_train, yx_val = y_x[train_idx], y_x[val_idx]\n",
    "        yy_train, yy_val = y_y[train_idx], y_y[val_idx]\n",
    "        \n",
    "        w_train = None if sample_weights is None else sample_weights[train_idx]\n",
    "        \n",
    "        # Baseline predictions for this fold\n",
    "        bx_val = base_x[val_idx] if base_x is not None else np.zeros(len(val_idx))\n",
    "        by_val = base_y[val_idx] if base_y is not None else np.zeros(len(val_idx))\n",
    "        \n",
    "        # --- LightGBM ---\n",
    "        if use_lgb:\n",
    "            print(\"\\nTraining LightGBM...\")\n",
    "            train_data_x = lgb.Dataset(X_train, label=yx_train, weight=w_train)\n",
    "            val_data_x = lgb.Dataset(X_val, label=yx_val, reference=train_data_x)\n",
    "            \n",
    "            lgb_x = lgb.train(\n",
    "                LGBM_PARAMS,\n",
    "                train_data_x,\n",
    "                num_boost_round=3000,\n",
    "                valid_sets=[train_data_x, val_data_x],\n",
    "                valid_names=['train', 'valid'],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=300, verbose=False),\n",
    "                    lgb.log_evaluation(period=500)\n",
    "                ]\n",
    "            )\n",
    "            models_x_lgb.append(lgb_x)\n",
    "            \n",
    "            train_data_y = lgb.Dataset(X_train, label=yy_train, weight=w_train)\n",
    "            val_data_y = lgb.Dataset(X_val, label=yy_val, reference=train_data_y)\n",
    "            \n",
    "            lgb_y = lgb.train(\n",
    "                LGBM_PARAMS,\n",
    "                train_data_y,\n",
    "                num_boost_round=3000,\n",
    "                valid_sets=[train_data_y, val_data_y],\n",
    "                valid_names=['train', 'valid'],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=300, verbose=False),\n",
    "                    lgb.log_evaluation(period=500)\n",
    "                ]\n",
    "            )\n",
    "            models_y_lgb.append(lgb_y)\n",
    "            \n",
    "            pred_x_lgb = lgb_x.predict(X_val)\n",
    "            pred_y_lgb = lgb_y.predict(X_val)\n",
    "        \n",
    "        # --- XGBoost ---\n",
    "        if use_xgb:\n",
    "            print(\"\\nTraining XGBoost...\")\n",
    "            dtrain_x = xgb.DMatrix(X_train, label=yx_train, weight=w_train)\n",
    "            dval_x = xgb.DMatrix(X_val, label=yx_val)\n",
    "            \n",
    "            xgb_x = xgb.train(\n",
    "                XGB_PARAMS,\n",
    "                dtrain_x,\n",
    "                num_boost_round=3000,\n",
    "                evals=[(dtrain_x, 'train'), (dval_x, 'valid')],\n",
    "                early_stopping_rounds=300,\n",
    "                verbose_eval=500\n",
    "            )\n",
    "            models_x_xgb.append(xgb_x)\n",
    "            \n",
    "            dtrain_y = xgb.DMatrix(X_train, label=yy_train, weight=w_train)\n",
    "            dval_y = xgb.DMatrix(X_val, label=yy_val)\n",
    "            \n",
    "            xgb_y = xgb.train(\n",
    "                XGB_PARAMS,\n",
    "                dtrain_y,\n",
    "                num_boost_round=3000,\n",
    "                evals=[(dtrain_y, 'train'), (dval_y, 'valid')],\n",
    "                early_stopping_rounds=300,\n",
    "                verbose_eval=500\n",
    "            )\n",
    "            models_y_xgb.append(xgb_y)\n",
    "            \n",
    "            pred_x_xgb = xgb_x.predict(dval_x)\n",
    "            pred_y_xgb = xgb_y.predict(dval_y)\n",
    "        \n",
    "        # --- CatBoost ---\n",
    "        if use_cat:\n",
    "            print(\"\\nTraining CatBoost...\")\n",
    "            pool_train_x = CatPool(X_train, yx_train, weight=w_train)\n",
    "            pool_val_x = CatPool(X_val, yx_val)\n",
    "            \n",
    "            cat_x = CatBoostRegressor(**CAT_PARAMS)\n",
    "            cat_x.fit(pool_train_x, eval_set=pool_val_x, verbose=500)\n",
    "            models_x_cat.append(cat_x)\n",
    "            \n",
    "            pool_train_y = CatPool(X_train, yy_train, weight=w_train)\n",
    "            pool_val_y = CatPool(X_val, yy_val)\n",
    "            \n",
    "            cat_y = CatBoostRegressor(**CAT_PARAMS)\n",
    "            cat_y.fit(pool_train_y, eval_set=pool_val_y, verbose=500)\n",
    "            models_y_cat.append(cat_y)\n",
    "            \n",
    "            pred_x_cat = cat_x.predict(X_val)\n",
    "            pred_y_cat = cat_y.predict(X_val)\n",
    "        \n",
    "        # Ensemble predictions (average)\n",
    "        pred_x_res = np.zeros(len(val_idx))\n",
    "        pred_y_res = np.zeros(len(val_idx))\n",
    "        n_models = 0\n",
    "        \n",
    "        if use_lgb:\n",
    "            pred_x_res += pred_x_lgb\n",
    "            pred_y_res += pred_y_lgb\n",
    "            n_models += 1\n",
    "        if use_xgb:\n",
    "            pred_x_res += pred_x_xgb\n",
    "            pred_y_res += pred_y_xgb\n",
    "            n_models += 1\n",
    "        if use_cat:\n",
    "            pred_x_res += pred_x_cat\n",
    "            pred_y_res += pred_y_cat\n",
    "            n_models += 1\n",
    "        \n",
    "        pred_x_res /= n_models\n",
    "        pred_y_res /= n_models\n",
    "        \n",
    "        # Add back baseline and clip to field\n",
    "        pred_x_final = np.clip(pred_x_res + bx_val, 0.0, 120.0)\n",
    "        pred_y_final = np.clip(pred_y_res + by_val, 0.0, 53.3)\n",
    "        \n",
    "        # True targets (with baseline added back)\n",
    "        true_x = yx_val + bx_val\n",
    "        true_y = yy_val + by_val\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse_x = np.sqrt(mean_squared_error(true_x, pred_x_final))\n",
    "        rmse_y = np.sqrt(mean_squared_error(true_y, pred_y_final))\n",
    "        rmse_combined = np.sqrt(0.5 * (rmse_x**2 + rmse_y**2))\n",
    "        \n",
    "        print(f\"\\nFold {fold_idx} Results:\")\n",
    "        print(f\"  RMSE X: {rmse_x:.5f}\")\n",
    "        print(f\"  RMSE Y: {rmse_y:.5f}\")\n",
    "        print(f\"  Combined RMSE: {rmse_combined:.5f}\")\n",
    "        \n",
    "        fold_scores.append(rmse_combined)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cross-Validation Results:\")\n",
    "    print(f\"  Per-fold RMSE: {[f'{s:.5f}' for s in fold_scores]}\")\n",
    "    print(f\"  Mean RMSE: {np.mean(fold_scores):.5f} ± {np.std(fold_scores):.5f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'lgb_x': models_x_lgb if use_lgb else None,\n",
    "        'lgb_y': models_y_lgb if use_lgb else None,\n",
    "        'xgb_x': models_x_xgb if use_xgb else None,\n",
    "        'xgb_y': models_y_xgb if use_xgb else None,\n",
    "        'cat_x': models_x_cat if use_cat else None,\n",
    "        'cat_y': models_y_cat if use_cat else None,\n",
    "        'cv_scores': fold_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e31ff4b14214bbf850f398b3bef72e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weeks:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load all training data\n",
    "train_input, train_output = load_all_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Physics-based features...\")\n",
    "train_input = engineer_physics_features(train_input)\n",
    "\n",
    "print(\"2. Formation features...\")\n",
    "train_input = add_formation_features(train_input)\n",
    "\n",
    "print(\"3. Temporal features...\")\n",
    "train_input = add_temporal_features(train_input)\n",
    "\n",
    "print(\"4. Role features...\")\n",
    "train_input = add_role_features(train_input)\n",
    "\n",
    "print(\"5. Neighbor interaction features...\")\n",
    "neighbor_features = compute_neighbor_features(train_input, k_neighbors=K_NEIGHBORS, radius=RADIUS)\n",
    "\n",
    "print(f\"\\nFeature engineering complete. Total features: {train_input.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training rows (merge last observations with future targets)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TRAINING DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_df = create_training_rows(train_input, train_output)\n",
    "print(f\"Training rows before merging neighbors: {train_df.shape}\")\n",
    "\n",
    "# Merge neighbor features\n",
    "train_df = train_df.merge(neighbor_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "print(f\"Training rows after merging neighbors: {train_df.shape}\")\n",
    "\n",
    "# Calculate baseline predictions (constant acceleration)\n",
    "print(\"\\nCalculating baseline (constant acceleration model)...\")\n",
    "baseline_x, baseline_y = constant_acceleration_baseline(\n",
    "    train_df['x'].values,\n",
    "    train_df['y'].values,\n",
    "    train_df['velocity_x'].values,\n",
    "    train_df['velocity_y'].values,\n",
    "    train_df['delta_time'].values,\n",
    "    train_df['acceleration_x'].values,\n",
    "    train_df['acceleration_y'].values\n",
    ")\n",
    "\n",
    "train_df['baseline_x'] = baseline_x\n",
    "train_df['baseline_y'] = baseline_y\n",
    "\n",
    "# Calculate residuals (what the model needs to learn)\n",
    "train_df['residual_x'] = train_df['target_x'] - train_df['baseline_x']\n",
    "train_df['residual_y'] = train_df['target_y'] - train_df['baseline_y']\n",
    "\n",
    "# Baseline RMSE\n",
    "baseline_rmse_x = np.sqrt(mean_squared_error(train_df['target_x'], baseline_x))\n",
    "baseline_rmse_y = np.sqrt(mean_squared_error(train_df['target_y'], baseline_y))\n",
    "baseline_rmse = np.sqrt(0.5 * (baseline_rmse_x**2 + baseline_rmse_y**2))\n",
    "\n",
    "print(f\"\\nBaseline Performance:\")\n",
    "print(f\"  RMSE X: {baseline_rmse_x:.5f}\")\n",
    "print(f\"  RMSE Y: {baseline_rmse_y:.5f}\")\n",
    "print(f\"  Combined RMSE: {baseline_rmse:.5f}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bc970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING FEATURE MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define feature columns (exclude targets, IDs, etc.)\n",
    "exclude_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'id', 'last_frame_id',\n",
    "                'target_x', 'target_y', 'residual_x', 'residual_y',\n",
    "                'player_name', 'player_birth_date', 'player_position', \n",
    "                'player_height', 'player_role', 'player_side', 'play_direction',\n",
    "                'absolute_yardline_number', 'num_frames_output', 'player_to_predict']\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Clean data\n",
    "train_clean = train_df.dropna(subset=['residual_x', 'residual_y'] + feature_cols[:50]).reset_index(drop=True)\n",
    "print(f\"Rows after dropping NaN: {len(train_clean):,}\")\n",
    "\n",
    "# Replace inf with NaN and fill with 0\n",
    "train_clean[feature_cols] = train_clean[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = train_clean[feature_cols].astype(np.float32).values\n",
    "y_x = train_clean['residual_x'].astype(np.float32).values\n",
    "y_y = train_clean['residual_y'].astype(np.float32).values\n",
    "baseline_x_arr = train_clean['baseline_x'].astype(np.float32).values\n",
    "baseline_y_arr = train_clean['baseline_y'].astype(np.float32).values\n",
    "\n",
    "# Sample weights (prioritize longer horizons)\n",
    "sample_weights = (1.0 + HORIZON_WEIGHT * train_clean['delta_frames'].clip(1, 5)).astype(np.float32).values\n",
    "\n",
    "# Groups for GroupKFold (prevent leakage)\n",
    "groups = (train_clean['game_id'].astype(str) + '_' + \n",
    "          train_clean['play_id'].astype(str) + '_' + \n",
    "          train_clean['nfl_id'].astype(str)).values\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  y_x: {y_x.shape}\")\n",
    "print(f\"  y_y: {y_y.shape}\")\n",
    "print(f\"  Unique groups: {len(np.unique(groups)):,}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ENSEMBLE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = train_ensemble_models(\n",
    "    X, y_x, y_y,\n",
    "    sample_weights=sample_weights,\n",
    "    groups=groups,\n",
    "    base_x=baseline_x_arr,\n",
    "    base_y=baseline_y_arr,\n",
    "    use_lgb=True,\n",
    "    use_xgb=True,\n",
    "    use_cat=True\n",
    ")\n",
    "\n",
    "# Save models\n",
    "print(\"\\nSaving models...\")\n",
    "with open(SAVE_DIR / 'ensemble_models.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'models': models,\n",
    "        'feature_cols': feature_cols,\n",
    "        'baseline_rmse': baseline_rmse\n",
    "    }, f)\n",
    "print(f\"Models saved to: {SAVE_DIR / 'ensemble_models.pkl'}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c79007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(models_dict, X_test, baseline_x, baseline_y):\n",
    "    \"\"\"Make predictions using ensemble of models\"\"\"\n",
    "    \n",
    "    pred_x_res = np.zeros(len(X_test))\n",
    "    pred_y_res = np.zeros(len(X_test))\n",
    "    n_models = 0\n",
    "    \n",
    "    # LightGBM predictions\n",
    "    if models_dict['lgb_x'] is not None:\n",
    "        lgb_pred_x = np.mean([m.predict(X_test) for m in models_dict['lgb_x']], axis=0)\n",
    "        lgb_pred_y = np.mean([m.predict(X_test) for m in models_dict['lgb_y']], axis=0)\n",
    "        pred_x_res += lgb_pred_x\n",
    "        pred_y_res += lgb_pred_y\n",
    "        n_models += 1\n",
    "    \n",
    "    # XGBoost predictions\n",
    "    if models_dict['xgb_x'] is not None:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        xgb_pred_x = np.mean([m.predict(dtest) for m in models_dict['xgb_x']], axis=0)\n",
    "        xgb_pred_y = np.mean([m.predict(dtest) for m in models_dict['xgb_y']], axis=0)\n",
    "        pred_x_res += xgb_pred_x\n",
    "        pred_y_res += xgb_pred_y\n",
    "        n_models += 1\n",
    "    \n",
    "    # CatBoost predictions\n",
    "    if models_dict['cat_x'] is not None:\n",
    "        cat_pred_x = np.mean([m.predict(X_test) for m in models_dict['cat_x']], axis=0)\n",
    "        cat_pred_y = np.mean([m.predict(X_test) for m in models_dict['cat_y']], axis=0)\n",
    "        pred_x_res += cat_pred_x\n",
    "        pred_y_res += cat_pred_y\n",
    "        n_models += 1\n",
    "    \n",
    "    # Average ensemble predictions\n",
    "    pred_x_res /= n_models\n",
    "    pred_y_res /= n_models\n",
    "    \n",
    "    # Add baseline and clip to field boundaries\n",
    "    pred_x = np.clip(pred_x_res + baseline_x, 0.0, 120.0)\n",
    "    pred_y = np.clip(pred_y_res + baseline_y, 0.0, 53.3)\n",
    "    \n",
    "    return pred_x, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_input = pd.read_csv(BASEDIR / 'test_input.csv')\n",
    "test_template = pd.read_csv(BASEDIR / 'test.csv')\n",
    "\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "print(f\"Test template shape: {test_template.shape}\")\n",
    "\n",
    "# Apply same feature engineering to test data\n",
    "print(\"\\nApplying feature engineering to test data...\")\n",
    "test_input = engineer_physics_features(test_input)\n",
    "test_input = add_formation_features(test_input)\n",
    "test_input = add_temporal_features(test_input)\n",
    "test_input = add_role_features(test_input)\n",
    "\n",
    "print(\"Computing neighbor features for test data...\")\n",
    "neighbor_features_test = compute_neighbor_features(test_input, k_neighbors=K_NEIGHBORS, radius=RADIUS)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "print(\"\\nPreparing test features...\")\n",
    "\n",
    "# Get last frame observations\n",
    "test_last = (test_input.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "                       .groupby(['game_id', 'play_id', 'nfl_id'], as_index=False)\n",
    "                       .tail(1)\n",
    "                       .rename(columns={'frame_id': 'last_frame_id'}))\n",
    "\n",
    "# Merge with test template\n",
    "test_df = test_template.merge(test_last, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "test_df = test_df.merge(neighbor_features_test, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
    "\n",
    "# Calculate time deltas\n",
    "test_df['delta_frames'] = (test_df['frame_id'] - test_df['last_frame_id']).clip(lower=0).astype(float)\n",
    "test_df['delta_time'] = test_df['delta_frames'] / 10.0\n",
    "\n",
    "# Calculate baseline predictions for test\n",
    "baseline_x_test, baseline_y_test = constant_acceleration_baseline(\n",
    "    test_df['x'].values,\n",
    "    test_df['y'].values,\n",
    "    test_df['velocity_x'].values,\n",
    "    test_df['velocity_y'].values,\n",
    "    test_df['delta_time'].values,\n",
    "    test_df['acceleration_x'].values,\n",
    "    test_df['acceleration_y'].values\n",
    ")\n",
    "\n",
    "# Ensure all feature columns exist\n",
    "for col in feature_cols:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0.0\n",
    "\n",
    "# Clean test features\n",
    "test_df[feature_cols] = test_df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "X_test = test_df[feature_cols].astype(np.float32).values\n",
    "\n",
    "print(f\"Test feature matrix shape: {X_test.shape}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pred_x, pred_y = predict_ensemble(models, X_test, baseline_x_test, baseline_y_test)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': (test_df['game_id'].astype(str) + '_' +\n",
    "           test_df['play_id'].astype(str) + '_' +\n",
    "           test_df['nfl_id'].astype(str) + '_' +\n",
    "           test_df['frame_id'].astype(str)),\n",
    "    'x': pred_x,\n",
    "    'y': pred_y\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission_path = SAVE_DIR / 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nFirst few predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337a4df",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Key Improvements Implemented:\n",
    "\n",
    "1. **Enhanced Feature Engineering**:\n",
    "   - Physics-based features (momentum, kinetic energy, force projections)\n",
    "   - Ball-relative coordinate system (parallel/perpendicular velocities)\n",
    "   - Temporal patterns (lag features, rolling statistics, trajectory curvature)\n",
    "   - Neighbor interactions (weighted aggregation of nearby players)\n",
    "   - Formation features (team centroids, spatial relationships)\n",
    "\n",
    "2. **Multi-Model Ensemble**:\n",
    "   - LightGBM (fast, efficient gradient boosting)\n",
    "   - XGBoost (robust, handles complex patterns)\n",
    "   - CatBoost (strong baseline performance)\n",
    "   - Average ensemble for robustness\n",
    "\n",
    "3. **Residual Learning**:\n",
    "   - Baseline: constant acceleration physics model\n",
    "   - Models learn to correct baseline errors\n",
    "   - More stable and interpretable than direct position prediction\n",
    "\n",
    "4. **Advanced Validation**:\n",
    "   - GroupKFold to prevent data leakage\n",
    "   - Horizon-weighted loss (prioritize longer predictions)\n",
    "   - Time-aware splitting\n",
    "\n",
    "### Expected Performance:\n",
    "- The baseline constant acceleration model achieves ~1.7-2.0 RMSE\n",
    "- Our ensemble approach should improve this to **~1.3-1.5 RMSE** or better\n",
    "- The multi-model ensemble provides robustness against overfitting\n",
    "\n",
    "### Potential Further Improvements:\n",
    "1. **Neural Network Component**: Add LSTM/Transformer for sequence modeling\n",
    "2. **Role-Specific Models**: Train separate models for QB, WR, DB, etc.\n",
    "3. **Play Context**: Incorporate down, distance, score differential\n",
    "4. **Advanced Physics**: Model player deceleration, reaction times\n",
    "5. **Hyperparameter Tuning**: Bayesian optimization for each model\n",
    "6. **Stacking**: Use meta-learner to combine model predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
