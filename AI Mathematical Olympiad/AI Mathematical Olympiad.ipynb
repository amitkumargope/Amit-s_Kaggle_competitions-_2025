{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00c7531",
   "metadata": {},
   "source": [
    "# AI Mathematical Olympiad (AIMO) Progress Prize 3\n",
    "\n",
    "## Competition Overview\n",
    "\n",
    "This is the **third AIMO Progress Prize competition**, building upon:\n",
    "- **First Prize (July 2024)**: Won by Project Numina\n",
    "- **Second Prize (April 2025)**: Won by Nvidia's NemoSkills team\n",
    "\n",
    "### Key Features of AIMO3:\n",
    "- **Significantly increased problem difficulty**: 110 problems spanning algebra, combinatorics, geometry, and number theory\n",
    "- **Problem difficulty range**: From National Olympiad level to IMO standard (pinnacle of high school mathematics)\n",
    "- **New submission format**: Dual-answer system with penalized accuracy scoring\n",
    "- **Expanded prize pool**: Part of $10mn AIMO Prize fund\n",
    "- **Industry-leading compute resources** for participants\n",
    "- **Auxiliary prizes** to reward community contributions\n",
    "- **Updated rules** for using open-source LLMs\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "The ability to reason mathematically is a critical milestone for AI. Current AI capabilities show a significant gap between closed-source and open-source models:\n",
    "- Closed-source models achieved **gold medal performance** at 2025 IMO\n",
    "- OpenAI x AIMO eval (March 2025): Commercial models solved **50/50 AIMO2 problems**\n",
    "- Highest Kaggle score was only **34/50**, demonstrating the gap\n",
    "\n",
    "AIMO3 is designed to **accelerate open-source progress** and close this gap.\n",
    "\n",
    "### Problem Design\n",
    "\n",
    "- **100% original problems**: Created by international team of problem solvers\n",
    "- **Zero risk of train-test contamination**\n",
    "- **\"AI hard\" design**: Tested against current open LLMs' capabilities\n",
    "- **Genuine mathematical reasoning required**: Answer-only testing is robust\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "### Public Leaderboard (During Competition)\n",
    "- Evaluated on **public test set only**\n",
    "- Score = **unnormalized accuracy** (number of correct answers)\n",
    "\n",
    "### Private Leaderboard (Final Evaluation)\n",
    "- Submission notebooks run **twice** over private test set\n",
    "- Predictions concatenated into single submission file\n",
    "- **Penalized accuracy scoring**:\n",
    "  - **Both answers correct**: 1.0 point\n",
    "  - **One answer correct**: 0.5 points\n",
    "  - **Neither answer correct**: 0.0 points\n",
    "- **Overall score**: Sum of scores for all problems\n",
    "\n",
    "### Answer Format\n",
    "- All ground-truth labels are **integers between 0 and 99999** (inclusive)\n",
    "- Any modulo calculation is **explicitly stated** in problem statement\n",
    "- **No implicit modulo 1000** (change from AIMO1 and AIMO2)\n",
    "\n",
    "---\n",
    "\n",
    "## Approach\n",
    "\n",
    "This notebook implements a solution using:\n",
    "1. **Data exploration and preprocessing**\n",
    "2. **LLM-based mathematical reasoning** with prompt engineering\n",
    "3. **Multiple solving strategies** to generate diverse solutions\n",
    "4. **Dual-answer submission** system for robust evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78697278",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb42bb",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "Load the competition data files. The data directory should contain:\n",
    "- `train.csv`: Training problems with solutions\n",
    "- `test.csv`: Test problems to solve\n",
    "- `sample_submission.csv`: Submission format template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path - Update this to your local data directory\n",
    "DATA_PATH = Path(\"/kaggle/input/ai-mathematical-olympiad-progress-prize-3\")\n",
    "\n",
    "# Alternative path for local development\n",
    "# DATA_PATH = Path(\"C:/Users/gopeami/OneDrive - Vesuvius/Desktop/PhD13- 2025-2026/ML Practice/Kaggle Compettition/AI Mathematical Olympiad/ai-mathematical-olympiad-progress-prize-3\")\n",
    "\n",
    "# Check if data directory exists\n",
    "if DATA_PATH.exists():\n",
    "    print(f\"Data directory found: {DATA_PATH}\")\n",
    "    print(f\"\\nFiles in directory:\")\n",
    "    for file in DATA_PATH.iterdir():\n",
    "        print(f\"  - {file.name}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Data directory not found: {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH to your local data directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    train_df = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "    test_df = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "    sample_submission = pd.read_csv(DATA_PATH / \"sample_submission.csv\")\n",
    "    \n",
    "    print(\"‚úì Data loaded successfully!\\n\")\n",
    "    print(f\"Training set shape: {train_df.shape}\")\n",
    "    print(f\"Test set shape: {test_df.shape}\")\n",
    "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"Creating dummy data for demonstration...\")\n",
    "    \n",
    "    # Create dummy dataframes for development\n",
    "    train_df = pd.DataFrame({\n",
    "        'id': range(5),\n",
    "        'problem': ['Sample problem ' + str(i) for i in range(5)],\n",
    "        'answer': [12345, 67890, 11111, 22222, 33333]\n",
    "    })\n",
    "    test_df = pd.DataFrame({\n",
    "        'id': range(10),\n",
    "        'problem': ['Test problem ' + str(i) for i in range(10)]\n",
    "    })\n",
    "    sample_submission = pd.DataFrame({\n",
    "        'id': range(10),\n",
    "        'answer': [0] * 10\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05528951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the training data\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING DATA OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "if 'answer' in train_df.columns:\n",
    "    print(f\"\\n\\nAnswer Statistics:\")\n",
    "    print(train_df['answer'].describe())\n",
    "    print(f\"\\nAnswer range: {train_df['answer'].min()} to {train_df['answer'].max()}\")\n",
    "    print(f\"All answers in valid range (0-99999): {train_df['answer'].between(0, 99999).all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db29d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the test data\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST DATA OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nColumns: {list(test_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(f\"\\n\\nTotal problems to solve: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7037768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sample submission format\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE SUBMISSION FORMAT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nColumns: {list(sample_submission.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(sample_submission.head(10))\n",
    "\n",
    "print(f\"\\n\\nSubmission requirements:\")\n",
    "print(f\"  - Each problem ID should appear exactly once\")\n",
    "print(f\"  - Answers must be integers between 0 and 99999\")\n",
    "print(f\"  - Notebook will be run TWICE and predictions concatenated for dual-answer scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceda082",
   "metadata": {},
   "source": [
    "## 3. Mathematical Reasoning Framework\n",
    "\n",
    "This section implements the core problem-solving approach using:\n",
    "- **Prompt Engineering**: Structured prompts for mathematical problem solving\n",
    "- **Answer Extraction**: Robust parsing of numerical answers\n",
    "- **Multiple Strategies**: Different solving approaches for diversity\n",
    "\n",
    "### Key Strategies:\n",
    "1. **Chain-of-Thought Reasoning**: Step-by-step mathematical deduction\n",
    "2. **Multiple Approaches**: Algebra, combinatorics, geometry, number theory\n",
    "3. **Verification**: Answer validation and range checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathProblemSolver:\n",
    "    \"\"\"\n",
    "    Mathematical problem solver using structured reasoning approaches.\n",
    "    Designed for AIMO3 competition requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.valid_range = (0, 99999)\n",
    "        \n",
    "    def extract_answer(self, text: str) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Extract numerical answer from solution text.\n",
    "        Returns integer between 0 and 99999, or None if not found.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        \n",
    "        # Look for common answer patterns\n",
    "        patterns = [\n",
    "            r'(?:answer|solution|result)(?:\\s+is)?(?:\\s*:)?\\s*(\\d+)',\n",
    "            r'(?:equals?|=)\\s*(\\d+)',\n",
    "            r'(?:therefore|thus|hence)(?:,)?\\s*(?:the answer is)?\\s*(\\d+)',\n",
    "            r'\\b(\\d+)\\s*(?:is the answer|is the solution)',\n",
    "            r'final answer:\\s*(\\d+)',\n",
    "            r'\\\\boxed\\{(\\d+)\\}',  # LaTeX boxed answer\n",
    "            r'\\$(\\d+)\\$',  # LaTeX math mode\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text.lower())\n",
    "            if match:\n",
    "                try:\n",
    "                    answer = int(match.group(1))\n",
    "                    if self.valid_range[0] <= answer <= self.valid_range[1]:\n",
    "                        return answer\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "        \n",
    "        # Fallback: look for last number in text\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', text)\n",
    "        if numbers:\n",
    "            for num in reversed(numbers):\n",
    "                try:\n",
    "                    answer = int(num)\n",
    "                    if self.valid_range[0] <= answer <= self.valid_range[1]:\n",
    "                        return answer\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def validate_answer(self, answer: Optional[int]) -> int:\n",
    "        \"\"\"\n",
    "        Validate and sanitize answer.\n",
    "        Returns valid integer or 0 as fallback.\n",
    "        \"\"\"\n",
    "        if answer is None:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            answer = int(answer)\n",
    "            # Clamp to valid range\n",
    "            return max(self.valid_range[0], min(answer, self.valid_range[1]))\n",
    "        except (ValueError, TypeError):\n",
    "            return 0\n",
    "    \n",
    "    def create_prompt(self, problem: str, strategy: str = \"standard\") -> str:\n",
    "        \"\"\"\n",
    "        Create structured prompt for mathematical problem solving.\n",
    "        \n",
    "        Args:\n",
    "            problem: The mathematical problem statement\n",
    "            strategy: Solving strategy ('standard', 'detailed', 'creative')\n",
    "        \"\"\"\n",
    "        if strategy == \"detailed\":\n",
    "            prompt = f\"\"\"You are a world-class mathematician solving an International Mathematical Olympiad problem.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Please solve this problem step-by-step:\n",
    "1. Understand the problem and identify what is being asked\n",
    "2. Identify the mathematical concepts and techniques needed\n",
    "3. Work through the solution systematically with clear reasoning\n",
    "4. Verify your answer makes sense\n",
    "5. State the final numerical answer clearly\n",
    "\n",
    "Remember: The answer must be an integer between 0 and 99999.\n",
    "If modulo arithmetic is needed, it will be explicitly stated in the problem.\n",
    "\n",
    "Solution:\"\"\"\n",
    "        \n",
    "        elif strategy == \"creative\":\n",
    "            prompt = f\"\"\"You are solving a challenging mathematical olympiad problem that requires creative thinking.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Approach this problem by:\n",
    "1. Consider multiple solution strategies\n",
    "2. Look for patterns or special properties\n",
    "3. Use elegant mathematical insights\n",
    "4. Double-check your calculations\n",
    "5. Provide the final integer answer (0-99999)\n",
    "\n",
    "Solution:\"\"\"\n",
    "        \n",
    "        else:  # standard\n",
    "            prompt = f\"\"\"Solve the following mathematical problem and provide the numerical answer.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Provide a clear solution with the final answer as an integer between 0 and 99999.\n",
    "\n",
    "Solution:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def solve_problem_symbolic(self, problem: str) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Attempt to solve problem using symbolic computation (sympy).\n",
    "        This is a placeholder for integration with symbolic math libraries.\n",
    "        \"\"\"\n",
    "        # This would integrate with SymPy or similar libraries\n",
    "        # For now, return None to indicate symbolic solving not available\n",
    "        return None\n",
    "    \n",
    "    def solve_problem_numerical(self, problem: str) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Attempt to solve problem using numerical methods.\n",
    "        This is a placeholder for numerical computation approaches.\n",
    "        \"\"\"\n",
    "        # This would use NumPy, SciPy, or similar for numerical solutions\n",
    "        # For now, return None\n",
    "        return None\n",
    "\n",
    "# Initialize solver\n",
    "solver = MathProblemSolver()\n",
    "print(\"‚úì Mathematical Problem Solver initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the solver with a sample problem\n",
    "sample_problem = \"What is the sum of the first 100 positive integers?\"\n",
    "\n",
    "print(\"Testing MathProblemSolver with sample problem:\")\n",
    "print(f\"\\nProblem: {sample_problem}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Test prompt generation\n",
    "prompt = solver.create_prompt(sample_problem, strategy=\"detailed\")\n",
    "print(\"\\nGenerated Prompt (first 300 chars):\")\n",
    "print(prompt[:300] + \"...\")\n",
    "\n",
    "# Test answer extraction\n",
    "test_solutions = [\n",
    "    \"The answer is 5050\",\n",
    "    \"Therefore, the sum equals 5050.\",\n",
    "    \"Using the formula n(n+1)/2, we get 100(101)/2 = 5050\",\n",
    "    \"Final answer: \\\\boxed{5050}\",\n",
    "    \"The result is $5050$\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Testing Answer Extraction:\")\n",
    "for i, solution in enumerate(test_solutions, 1):\n",
    "    extracted = solver.extract_answer(solution)\n",
    "    print(f\"\\n{i}. Solution: '{solution}'\")\n",
    "    print(f\"   Extracted answer: {extracted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb8db8f",
   "metadata": {},
   "source": [
    "## 4. Solution Generation Pipeline\n",
    "\n",
    "This section implements the complete solution pipeline:\n",
    "\n",
    "1. **Problem Processing**: Load and prepare test problems\n",
    "2. **Multi-Strategy Solving**: Apply different solving approaches\n",
    "3. **Answer Aggregation**: Combine results from multiple strategies\n",
    "4. **Dual-Answer System**: Prepare for the two-run submission format\n",
    "\n",
    "### Dual-Answer Strategy:\n",
    "Since notebooks are run **twice** and predictions concatenated:\n",
    "- **Run 1**: Use primary solving strategy\n",
    "- **Run 2**: Use alternative strategy or randomized approach\n",
    "- This maximizes chances of getting at least one correct answer (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_llm_placeholder(problem: str, strategy: str = \"standard\") -> int:\n",
    "    \"\"\"\n",
    "    Placeholder for LLM-based solving.\n",
    "    \n",
    "    In actual implementation, this would:\n",
    "    1. Call an LLM API (OpenAI, Anthropic, Gemini, etc.)\n",
    "    2. Use the generated prompt\n",
    "    3. Extract the answer from the response\n",
    "    \n",
    "    For now, returns a random valid answer for demonstration.\n",
    "    \"\"\"\n",
    "    # This is where you would integrate with your LLM of choice\n",
    "    # Example with OpenAI API:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-4\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": solver.create_prompt(problem, strategy)}]\n",
    "    # )\n",
    "    # answer_text = response.choices[0].message.content\n",
    "    # return solver.extract_answer(answer_text)\n",
    "    \n",
    "    # Placeholder: return a deterministic hash-based answer for demo\n",
    "    import hashlib\n",
    "    hash_val = int(hashlib.md5(problem.encode()).hexdigest(), 16)\n",
    "    return hash_val % 100000  # Keep in valid range\n",
    "\n",
    "def solve_problem_multi_strategy(problem_id: int, problem: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Solve a problem using multiple strategies and return all answers.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with strategy names as keys and answers as values\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Strategy 1: Standard approach\n",
    "    results['standard'] = solve_with_llm_placeholder(problem, strategy=\"standard\")\n",
    "    \n",
    "    # Strategy 2: Detailed step-by-step\n",
    "    results['detailed'] = solve_with_llm_placeholder(problem, strategy=\"detailed\")\n",
    "    \n",
    "    # Strategy 3: Creative approach\n",
    "    results['creative'] = solve_with_llm_placeholder(problem, strategy=\"creative\")\n",
    "    \n",
    "    # Strategy 4: Symbolic computation (if available)\n",
    "    symbolic_answer = solver.solve_problem_symbolic(problem)\n",
    "    if symbolic_answer is not None:\n",
    "        results['symbolic'] = symbolic_answer\n",
    "    \n",
    "    # Strategy 5: Numerical methods (if available)\n",
    "    numerical_answer = solver.solve_problem_numerical(problem)\n",
    "    if numerical_answer is not None:\n",
    "        results['numerical'] = numerical_answer\n",
    "    \n",
    "    return results\n",
    "\n",
    "def select_best_answer(results: Dict[str, int]) -> int:\n",
    "    \"\"\"\n",
    "    Select the best answer from multiple strategies.\n",
    "    \n",
    "    Strategy: Use majority voting, or select most frequent answer.\n",
    "    Fallback to first available answer.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return 0\n",
    "    \n",
    "    # Count frequency of each answer\n",
    "    from collections import Counter\n",
    "    answer_counts = Counter(results.values())\n",
    "    \n",
    "    # Return most common answer\n",
    "    most_common = answer_counts.most_common(1)[0][0]\n",
    "    return solver.validate_answer(most_common)\n",
    "\n",
    "print(\"‚úì Solution generation pipeline functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aaefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the multi-strategy solving on a sample problem\n",
    "if len(test_df) > 0:\n",
    "    sample_idx = 0\n",
    "    sample_id = test_df.iloc[sample_idx]['id']\n",
    "    sample_problem = test_df.iloc[sample_idx]['problem']\n",
    "    \n",
    "    print(f\"Testing multi-strategy solving:\")\n",
    "    print(f\"\\nProblem ID: {sample_id}\")\n",
    "    print(f\"Problem: {sample_problem[:200]}...\" if len(sample_problem) > 200 else f\"Problem: {sample_problem}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Solve using multiple strategies\n",
    "    strategy_results = solve_problem_multi_strategy(sample_id, sample_problem)\n",
    "    \n",
    "    print(\"\\nResults from different strategies:\")\n",
    "    for strategy, answer in strategy_results.items():\n",
    "        print(f\"  {strategy:15s}: {answer}\")\n",
    "    \n",
    "    # Select best answer\n",
    "    best_answer = select_best_answer(strategy_results)\n",
    "    print(f\"\\n{'Selected answer':15s}: {best_answer}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb26ff",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions for All Test Problems\n",
    "\n",
    "Now we'll process all test problems and generate predictions. \n",
    "\n",
    "**Important Notes:**\n",
    "- In a production setting, replace `solve_with_llm_placeholder` with actual LLM API calls\n",
    "- Consider using local open-source models (Llama, Mistral, etc.) for Kaggle submission\n",
    "- Implement caching to avoid redundant API calls\n",
    "- Add error handling and retry logic for API failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6836544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_predictions(test_data: pd.DataFrame, \n",
    "                           use_multi_strategy: bool = True,\n",
    "                           verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate predictions for all test problems.\n",
    "    \n",
    "    Args:\n",
    "        test_data: DataFrame with 'id' and 'problem' columns\n",
    "        use_multi_strategy: If True, use multiple strategies and vote\n",
    "        verbose: If True, print progress\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'id' and 'answer' columns\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    total_problems = len(test_data)\n",
    "    \n",
    "    for idx, row in test_data.iterrows():\n",
    "        problem_id = row['id']\n",
    "        problem = row['problem']\n",
    "        \n",
    "        if verbose and (idx % 10 == 0 or idx == total_problems - 1):\n",
    "            print(f\"Processing problem {idx + 1}/{total_problems} (ID: {problem_id})...\")\n",
    "        \n",
    "        if use_multi_strategy:\n",
    "            # Use multiple strategies and select best answer\n",
    "            results = solve_problem_multi_strategy(problem_id, problem)\n",
    "            answer = select_best_answer(results)\n",
    "        else:\n",
    "            # Use single strategy\n",
    "            answer = solve_with_llm_placeholder(problem, strategy=\"standard\")\n",
    "            answer = solver.validate_answer(answer)\n",
    "        \n",
    "        predictions.append({\n",
    "            'id': problem_id,\n",
    "            'answer': answer\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "# Generate predictions for all test problems\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING PREDICTIONS FOR ALL TEST PROBLEMS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal problems to solve: {len(test_df)}\")\n",
    "print(\"\\nStarting prediction generation...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "predictions_df = generate_all_predictions(test_df, use_multi_strategy=True, verbose=True)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"‚úì Prediction generation complete!\")\n",
    "print(f\"\\nGenerated predictions for {len(predictions_df)} problems\")\n",
    "print(f\"\\nFirst few predictions:\")\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506259bf",
   "metadata": {},
   "source": [
    "## 6. Validate and Prepare Submission\n",
    "\n",
    "Final validation steps before creating submission file:\n",
    "1. Check all problem IDs are present\n",
    "2. Verify all answers are in valid range (0-99999)\n",
    "3. Ensure correct format matches sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks\n",
    "print(\"=\" * 80)\n",
    "print(\"SUBMISSION VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: All test IDs present\n",
    "test_ids = set(test_df['id'])\n",
    "pred_ids = set(predictions_df['id'])\n",
    "\n",
    "missing_ids = test_ids - pred_ids\n",
    "extra_ids = pred_ids - test_ids\n",
    "\n",
    "print(f\"\\n‚úì Check 1: Problem ID coverage\")\n",
    "print(f\"  Test problems: {len(test_ids)}\")\n",
    "print(f\"  Predictions: {len(pred_ids)}\")\n",
    "print(f\"  Missing IDs: {len(missing_ids)}\")\n",
    "print(f\"  Extra IDs: {len(extra_ids)}\")\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\"  ‚ö†Ô∏è Warning: Missing predictions for IDs: {missing_ids}\")\n",
    "if extra_ids:\n",
    "    print(f\"  ‚ö†Ô∏è Warning: Extra predictions for IDs: {extra_ids}\")\n",
    "\n",
    "# Check 2: Answer range validation\n",
    "print(f\"\\n‚úì Check 2: Answer range (0-99999)\")\n",
    "print(f\"  Min answer: {predictions_df['answer'].min()}\")\n",
    "print(f\"  Max answer: {predictions_df['answer'].max()}\")\n",
    "\n",
    "invalid_answers = predictions_df[\n",
    "    (predictions_df['answer'] < 0) | (predictions_df['answer'] > 99999)\n",
    "]\n",
    "print(f\"  Invalid answers: {len(invalid_answers)}\")\n",
    "\n",
    "if len(invalid_answers) > 0:\n",
    "    print(f\"  ‚ö†Ô∏è Warning: Found invalid answers:\")\n",
    "    print(invalid_answers)\n",
    "\n",
    "# Check 3: Data types\n",
    "print(f\"\\n‚úì Check 3: Data types\")\n",
    "print(f\"  ID type: {predictions_df['id'].dtype}\")\n",
    "print(f\"  Answer type: {predictions_df['answer'].dtype}\")\n",
    "\n",
    "# Check 4: No missing values\n",
    "print(f\"\\n‚úì Check 4: Missing values\")\n",
    "print(f\"  Missing IDs: {predictions_df['id'].isna().sum()}\")\n",
    "print(f\"  Missing answers: {predictions_df['answer'].isna().sum()}\")\n",
    "\n",
    "# Check 5: Format matches sample submission\n",
    "print(f\"\\n‚úì Check 5: Format verification\")\n",
    "print(f\"  Sample submission columns: {list(sample_submission.columns)}\")\n",
    "print(f\"  Predictions columns: {list(predictions_df.columns)}\")\n",
    "print(f\"  Columns match: {list(sample_submission.columns) == list(predictions_df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Validation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30dfbc",
   "metadata": {},
   "source": [
    "## 7. Create Submission File\n",
    "\n",
    "Create the final submission file in the correct format.\n",
    "\n",
    "**Important Reminder:**\n",
    "- The submission notebook will be run **TWICE** by Kaggle\n",
    "- Predictions from both runs will be concatenated\n",
    "- This enables the dual-answer scoring system:\n",
    "  - Both correct = 1.0 point\n",
    "  - One correct = 0.5 points  \n",
    "  - Neither correct = 0.0 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b484990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission DataFrame\n",
    "submission = predictions_df.copy()\n",
    "\n",
    "# Ensure correct column order\n",
    "submission = submission[['id', 'answer']]\n",
    "\n",
    "# Sort by ID for consistency\n",
    "submission = submission.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Display final submission\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUBMISSION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nLast 10 rows:\")\n",
    "print(submission.tail(10))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Save to CSV\n",
    "submission_path = \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Submission saved to: {submission_path}\")\n",
    "print(f\"  File size: {os.path.getsize(submission_path)} bytes\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e7608",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### What This Notebook Does:\n",
    "\n",
    "1. ‚úÖ **Loads competition data** (train.csv, test.csv, sample_submission.csv)\n",
    "2. ‚úÖ **Implements mathematical problem solver** with:\n",
    "   - Structured prompt engineering\n",
    "   - Answer extraction from text\n",
    "   - Validation and sanitization\n",
    "3. ‚úÖ **Multi-strategy solving approach**:\n",
    "   - Standard reasoning\n",
    "   - Detailed step-by-step\n",
    "   - Creative problem-solving\n",
    "   - Symbolic computation (placeholder)\n",
    "   - Numerical methods (placeholder)\n",
    "4. ‚úÖ **Generates predictions** for all test problems\n",
    "5. ‚úÖ **Validates submission** format and answer ranges\n",
    "6. ‚úÖ **Creates submission.csv** file\n",
    "\n",
    "### To Improve Performance:\n",
    "\n",
    "#### 1. **Integrate Real LLM**\n",
    "Replace `solve_with_llm_placeholder` with actual LLM calls:\n",
    "```python\n",
    "# Example with OpenAI\n",
    "import openai\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": solver.create_prompt(problem)}]\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. **Use Open-Source Models**\n",
    "For Kaggle submission, consider local models:\n",
    "- **Llama 3** (70B or 405B)\n",
    "- **Qwen 2.5** (Math specialized)\n",
    "- **DeepSeek-Math**\n",
    "- **Mistral Large**\n",
    "\n",
    "#### 3. **Implement Symbolic Math**\n",
    "Integrate SymPy for exact symbolic computation:\n",
    "```python\n",
    "import sympy as sp\n",
    "# Parse problem and solve symbolically\n",
    "```\n",
    "\n",
    "#### 4. **Add Verification**\n",
    "- Check answer validity within problem context\n",
    "- Verify calculations\n",
    "- Test edge cases\n",
    "\n",
    "#### 5. **Optimize Prompts**\n",
    "- Include few-shot examples\n",
    "- Add chain-of-thought prompting\n",
    "- Use problem-type specific prompts\n",
    "\n",
    "#### 6. **Ensemble Methods**\n",
    "- Combine multiple models\n",
    "- Use majority voting\n",
    "- Implement confidence scoring\n",
    "\n",
    "### Competition Strategy:\n",
    "\n",
    "**Dual-Run Optimization:**\n",
    "- Run 1: Use most reliable strategy\n",
    "- Run 2: Use diverse/alternative strategy\n",
    "- This maximizes partial credit (0.5 points)\n",
    "\n",
    "**Focus Areas:**\n",
    "1. **Algebra**: Pattern recognition, equation solving\n",
    "2. **Combinatorics**: Counting, probability\n",
    "3. **Geometry**: Spatial reasoning, coordinate geometry\n",
    "4. **Number Theory**: Modular arithmetic, divisibility\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Competition Page**: ai-mathematical-olympiad-progress-prize-3\n",
    "- **Reference Problems**: Check the PDF in data folder\n",
    "- **Previous Competitions**: Study AIMO1 and AIMO2 winning solutions\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with the competition! üéØ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
